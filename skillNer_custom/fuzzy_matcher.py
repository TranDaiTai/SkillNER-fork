from rapidfuzz.distance import JaroWinkler


class FuzzyPhraseMatcher:
    '''
    Fuzzy phrase-level matcher ƒë·ªÉ b·∫Øt typo trong C·ª§M SKILL / JOB TITLE.

    TRI·∫æT L√ù
    --------
    - Ch·ªâ fuzzy PHRASE (len > 1)
    - Fuzzy = s·ª≠a typo, KH√îNG ph·∫£i semantic match
    - Kh√¥ng cho m·ªü r·ªông / nu·ªët token
    - C√≥ token-level gate ƒë·ªÉ di·ªát false positive
    - Reject s·ªõm (cheap gate ‚Üí expensive gate)
    - Mutate tr·ª±c ti·∫øp text_obj (ƒë√∫ng thi·∫øt k·∫ø SkillNER)
    '''

    def __init__(
        self,
        skills_db,
        min_phrase_sim=0.92,
        min_token_sim=0.80
    ):
        self.min_phrase_sim = min_phrase_sim
        self.min_token_sim = min_token_sim
        self.skill_db = skills_db

        # üî• CACHE SKILL DATA (t·ªëi ∆∞u quan tr·ªçng)
        self.skill_cache = {
            skill_id: {
                "tokens": skill["high_surfce_forms"]["full"].lower().split(),
                "phrase": skill["high_surfce_forms"]["full"].lower(),
                "len": len(skill["high_surfce_forms"]["full"].split())
            }
            for skill_id, skill in skills_db.items()
        }

    def _span_is_matchable(self, text_obj, start, end):
        '''
        Span ch·ªâ ƒë∆∞·ª£c fuzzy n·∫øu to√†n b·ªô token c√≤n matchable
        '''
        for i in range(start, end):
            if not text_obj[i].is_matchable:
                return False
        return True

    def _token_level_pass(self, span_tokens, skill_tokens):
        '''
        Token-level gate (STRICT):
        M·ªñI token trong span ph·∫£i ƒë·ªß gi·ªëng token t∆∞∆°ng ·ª©ng trong skill
        '''
        for a, b in zip(span_tokens, skill_tokens):
            if JaroWinkler.similarity(a, b) < self.min_token_sim:
                return False
        return True

    def match(self, text_obj):
        '''
        Quy tr√¨nh match (theo th·ª© t·ª± t·ªëi ∆∞u):

        1. Span c√≤n matchable?
        2. Length gate (r·∫ª)
        3. First-token cheap fuzzy gate
        4. Phrase-level fuzzy
        5. Token-level strict gate
        '''
        matches = []
        tokens = [str(tok).lower() for tok in text_obj]
        text_len = len(tokens)

        for skill_id, info in self.skill_cache.items():
            skill_tokens = info["tokens"]
            skill_phrase = info["phrase"]
            skill_len = info["len"]

            # Ch·ªâ fuzzy phrase
            if skill_len <= 1:
                continue

            for i in range(text_len - skill_len + 1):
                j = i + skill_len

                # 1Ô∏è‚É£ Span ƒë√£ b·ªã matcher kh√°c chi·∫øm
                if not self._span_is_matchable(text_obj, i, j):
                    continue

                span_tokens = tokens[i:j]
                span_text = " ".join(span_tokens)

                # 2Ô∏è‚É£ Length gate (di·ªát punctuation / semantic drift)
                if abs(len(span_text) - len(skill_phrase)) > 3:
                    continue

                # 3Ô∏è‚É£ Cheap first-token gate
                if JaroWinkler.similarity(
                    span_tokens[0], skill_tokens[0]
                ) < 0.7:
                    continue

                # 4Ô∏è‚É£ Phrase-level fuzzy
                phrase_sim = JaroWinkler.similarity(
                    span_text, skill_phrase
                )
                if phrase_sim < self.min_phrase_sim:
                    continue

                # 5Ô∏è‚É£ Token-level strict gate (quan tr·ªçng nh·∫•t)
                if not self._token_level_pass(span_tokens, skill_tokens):
                    continue

                # ‚úÖ MATCH
                matches.append({
                    "skill_id": skill_id,
                    "doc_node_id": list(range(i, j)),
                    "doc_node_value": span_text,
                    "type": "fuzzy",
                    "score": round(phrase_sim, 3)
                })

                # ƒê√°nh d·∫•u token ƒë√£ d√πng
                for k in range(i, j):
                    text_obj[k].is_matchable = False

        return matches
